{{- $root := . -}}
{{- $enabledPipelines := include "ingestion-pipeline.enabledPipelines" . | fromJson -}}

{{/* Create jobs for enabled pipelines */}}
{{- range $pipelineKey, $pipelineConfig := $enabledPipelines }}
{{- $jobName := include "ingestion-pipeline.pipelineJobName" (dict "pipelineKey" $pipelineKey) -}}
{{- $pipelineDict := dict "root" $root "pipelineKey" $pipelineKey "pipelineConfig" $pipelineConfig "pipelineName" $pipelineKey -}}
{{- $pipelineData := include "ingestion-pipeline.preparePipelineData" $pipelineDict -}}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ $jobName }}
  labels:
    {{- include "ingestion-pipeline.pipelineLabels" $pipelineDict | nindent 4 }}
  annotations:
    ingestion-pipeline.ai/pipeline-key: {{ $pipelineKey }}
    ingestion-pipeline.ai/pipeline-source: {{ $pipelineConfig.source }}
spec:
  template:
    metadata:
      labels:
        {{- include "ingestion-pipeline.pipelineLabels" $pipelineDict | nindent 8 }}
      annotations:
        ingestion-pipeline.ai/pipeline-key: {{ $pipelineKey }}
    spec:
      initContainers:
        - name: wait-for-pipeline
          image: "image-registry.openshift-image-registry.svc:5000/openshift/tools:latest"
          command:
            - /bin/bash
            - -c
            - |
              set -e
              url="http://{{ include "ingestion-pipeline.fullname" $root }}/ping"
              echo "Waiting for $url..."
              until curl -ksf "$url"; do
                echo "Still waiting for $url ..."
                sleep 10
              done
              echo "Ingestion pipeline service ready"

              # Wait for LlamaStack to be available
              url="http://llamastack:8321/v1/models"
              echo "Waiting for $url..."
              until curl -ksf "$url"; do
                echo "Still waiting for $url ..."
                sleep 10
              done
              echo "Llama Stack server is running"

              # Warm up the embedding model to prevent meta tensor errors
              EMBEDDING_MODEL="{{ $pipelineConfig.embedding_model | default "all-MiniLM-L6-v2" }}"
              echo "Warming up embedding model: $EMBEDDING_MODEL..."
              until curl -sf -X POST "http://llamastack:8321/v1/inference/embeddings" \
                -H "Content-Type: application/json" \
                -d "{\"model_id\": \"$EMBEDDING_MODEL\", \"contents\": [\"warmup\"]}" \
                --max-time 60 | grep -q "embeddings"; do
                echo "  Waiting for embedding model to load..."
                sleep 5
              done
              echo "Embedding model ready!"
      containers:
        - name: create-{{ $pipelineKey }}-pipeline
          image: "image-registry.openshift-image-registry.svc:5000/openshift/tools:latest"
          imagePullPolicy: IfNotPresent
          env:
            - name: PIPELINE_KEY
              value: {{ $pipelineKey | quote }}
            - name: PIPELINE_SOURCE
              value: {{ $pipelineConfig.source | quote }}
            - name: PIPELINE_NAME
              value: {{ $pipelineConfig.name | quote }}
            - name: PIPELINE_VERSION
              value: {{ $pipelineConfig.version | quote }}
          command:
            - /bin/bash
            - -c
            - |
              echo "Creating pipeline: $PIPELINE_KEY ($PIPELINE_SOURCE)"
              echo "Pipeline data: {{ $pipelineData }}"
              
              curl -sfX 'POST' \
              'http://{{ include "ingestion-pipeline.fullname" $root }}/add' \
              -H 'accept: application/json' \
              -H 'Content-Type: application/json' \
              -d '{{ $pipelineData }}'
              
              echo "Pipeline $PIPELINE_KEY created successfully"
      restartPolicy: Never
  backoffLimit: 3
{{ end }}
